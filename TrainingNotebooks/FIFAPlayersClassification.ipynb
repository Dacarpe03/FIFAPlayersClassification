{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b87c7b",
   "metadata": {},
   "source": [
    "# FIFA Players Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e61cc03",
   "metadata": {},
   "source": [
    "This notebook contains the code to train models for the FIFA Classification problem based on their attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c2558",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf14b6",
   "metadata": {},
   "source": [
    "Libraries we will be using:\n",
    "- **tensorflow** and **keras**: APIs for neural network model construction and training\n",
    "- **numpy**: Numerical python library.\n",
    "- **pandas**: Will be used to work with dataframes from .csv files.\n",
    "- **sklearn**: Will be used to modify the labels of the data and do some statistical modifications.\n",
    "- **matplotlib**: Will be used to plot graphics.\n",
    "- **time**: To measure training time.\n",
    "- **datetime**: To get the datetime a new model is created and use that to create its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d511f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247c728",
   "metadata": {},
   "source": [
    "## Step 2: Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae528225",
   "metadata": {},
   "source": [
    "We first define the path to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf49033",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../Data\"\n",
    "\n",
    "ATT_FILE = f\"{DATA_FOLDER}/FootballPlayerPreparedCleanAttributes.csv\"\n",
    "IMPUTED_ATT_FILE = f\"{DATA_FOLDER}/ImputedFootballPlayerPreparedCleanAttributes.csv\"\n",
    "\n",
    "ONE_HOT_LABEL_FILE = f\"{DATA_FOLDER}/FootballPlayerOneHotEncodedClasses.csv\"\n",
    "IMPUTED_ONE_HOT_LABEL_FILE = f\"{DATA_FOLDER}/ImputedFootballPlayerPreparedCleanAttributes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c5b59",
   "metadata": {},
   "source": [
    "Specify training options:\n",
    "- **IMPUTED_DATA**: We will use file with data imputed\n",
    "- **TRAIN_RATE**: The amount of instances to use during training, it will also determine the dev and test instances number indirectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "492fb849",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPUTED_DATA = False    # Use imputed dataset\n",
    "TRAIN_RATE = 0.8        # The division of the data for training, validation and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591a7be",
   "metadata": {},
   "source": [
    "Because our data has more than 10000, we will be using 80-10-10 split in our dataset:\n",
    "- **80%** for training\n",
    "- **10%** for validation/dev test\n",
    "- **10%** for final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c30464b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instances_train: (12897, 17)\n",
      "labels_train: (12897, 4)\n",
      "instances_dev: (1612, 17)\n",
      "labels_dev: (1612, 4)\n"
     ]
    }
   ],
   "source": [
    "if IMPUTED_DATA:\n",
    "    attributes = pd.read_csv(ATT_IMPUTED_FILE)\n",
    "    labels = pd.read_csv(ONE_HOT_LABEL_IMPUTED_FILE)\n",
    "else:\n",
    "    attributes = pd.read_csv(ATT_FILE)\n",
    "    labels = pd.read_csv(ONE_HOT_LABEL_FILE)\n",
    "\n",
    "n_total_instances = attributes.shape[0]\n",
    "n_train_instances = int(n_total_instances * TRAIN_RATE)\n",
    "n_dev_instances = int((n_total_instances - n_train_instances)/2)\n",
    "\n",
    "instances_train = attributes.values[:n_train_instances]\n",
    "labels_train = labels.values[:n_train_instances]\n",
    "\n",
    "instances_dev = attributes.values[n_train_instances: n_train_instances + n_dev_instances]\n",
    "labels_dev = labels.values[n_train_instances: n_train_instances + n_dev_instances]\n",
    "\n",
    "print(\"instances_train:\", instances_train.shape)\n",
    "print(\"labels_train:\", labels_train.shape)\n",
    "print(\"instances_dev:\", instances_dev.shape)\n",
    "print(\"labels_dev:\",labels_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e971c",
   "metadata": {},
   "source": [
    "## Step 3: Initialize variables\n",
    "- **INPUT_SIZE**: The size of the training instances\n",
    "- **OUTPUT_SIZE**: The number of possible classes\n",
    "- **NUM_TRAINING_EXAMPLES**: The number of possible classes\n",
    "- **NUM_DEV_EXAMPLES**: The number of validation/dev test instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9dee8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = instances_train.shape[1]\n",
    "OUTPUT_SIZE = labels_train.shape[1]\n",
    "NUM_TRAINING_EXAMPLES = int(round(instances_train.shape[0]/1))\n",
    "NUM_DEV_EXAMPLES = int(round(instances_dev.shape[0]/1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e87f8",
   "metadata": {},
   "source": [
    "## Step 4: Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3930c053",
   "metadata": {},
   "source": [
    "- **LEARNING_RATE** $\\alpha$: The step size in the learning process\n",
    "- **BATCH_SIZE**: The size of each batch\n",
    "- **N_HIDDEN**: Number of neurons in the hidden.\n",
    "\n",
    "The learning process stops when it reaches 20,000 learning iterations. An iteration comprises presenting the network *batch_size* learning examples and then adjusting the weights (parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c7ff322",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1000\n",
    "LEARNING_RATE = 0.1\n",
    "BATCH_SIZE = 128\n",
    "ACTIVATION = \"tanh\"\n",
    "N_HIDDEN = [128, 64, 32]\n",
    "LOSS = tf.keras.losses.categorical_crossentropy\n",
    "OPTIMIZER = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
    "METRICS = [\"categorical_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caedd6ca",
   "metadata": {},
   "source": [
    "## Step 5: Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a55e794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def architecture1():\n",
    "    model = keras.Sequential(name=\"Model\")\n",
    "    model.add(keras.layers.InputLayer(input_shape=(INPUT_SIZE,), batch_size=None))\n",
    "    for neurons in N_HIDDEN:\n",
    "        model.add(keras.layers.Dense(neurons, activation=ACTIVATION))\n",
    "    model.add(keras.layers.Dense(OUTPUT_SIZE, activation=\"softmax\"))\n",
    "    \n",
    "    model_name = \"D512\"\n",
    "    return model, model_name\n",
    "\n",
    "def architecture2():\n",
    "    model = keras.Sequential(name=\"Model\")\n",
    "    model.add(keras.layers.InputLayer(input_shape=(INPUT_SIZE,), batch_size=None))\n",
    "    for neurons in N_HIDDEN:\n",
    "        model.add(keras.layers.Dense(neurons, activation=ACTIVATION))\n",
    "    model.add(keras.layers.Dense(OUTPUT_SIZE, activation=\"softmax\"))\n",
    "    \n",
    "    model_name = \"D128_D64_D32\"\n",
    "    return model, model_name\n",
    "\n",
    "def compile_model(model):\n",
    "    my_model.compile(loss=LOSS,\n",
    "                     optimizer=OPTIMIZER,\n",
    "                     metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model, model_name = architecture2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9caedf",
   "metadata": {},
   "source": [
    "## Step 6: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "history = my_model.fit(instances_train,\n",
    "                       labels_train,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       epochs=N_EPOCHS,\n",
    "                       verbose=1,\n",
    "                       validation_data=(instances_dev, labels_dev))\n",
    "end_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5cc2c",
   "metadata": {},
   "source": [
    "## Step 7: Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b566a5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results=pd.DataFrame(history.history)\n",
    "results.plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.xlabel (\"Epochs\")\n",
    "plt.ylabel (\"Accuracy - Mean Log Loss\")\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70714e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = round(results.categorical_accuracy.values[-1:][0]*100, 2)\n",
    "val_accuracy = round(results.val_categorical_accuracy.values[-1:][0]*100, 2)\n",
    "training_error = round((1 - results.categorical_accuracy.values[-1:][0])*100, 2)\n",
    "test_error = round((1 - results.val_categorical_accuracy.values[-1:][0])*100, 2)\n",
    "human_error = 5\n",
    "bias = round(training_error - human_error, 2)\n",
    "variance = round(test_error - training_error, 2)\n",
    "training_time = round(end_time - start_time)\n",
    "print(\"Error (training):\", training_error, \"%\")\n",
    "print(\"Error (test):\", test_error, \"%\")\n",
    "print(\"Bias:\", bias)\n",
    "print(\"Variance:\", variance)\n",
    "print(\"Time: \", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba84dc10",
   "metadata": {},
   "source": [
    "## Step 8: Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FOLDER = \"../Results\"\n",
    "RESULTS_FILE = f\"{RESULTS_FOLDER}/models_performance.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(RESULTS_FILE, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\n",
    "    'model_name',\n",
    "    'epochs',\n",
    "    'learning_rate',\n",
    "    'batch_size',\n",
    "    'activation',\n",
    "    'loss',\n",
    "    'optimizer',\n",
    "    'regularization',\n",
    "    'train_accuracy',\n",
    "    'validation_accuracy',\n",
    "    'bias',\n",
    "    'variance',\n",
    "    'training_time'\n",
    "]\n",
    "\n",
    "new_result = [[\n",
    "    model_name,\n",
    "    N_EPOCHS,\n",
    "    LEARNING_RATE,\n",
    "    BATCH_SIZE,\n",
    "    ACTIVATION,\n",
    "    \"categorical_crossentropy\",\n",
    "    \"SGD\",\n",
    "    \"\",\n",
    "    training_accuracy,\n",
    "    val_accuracy,\n",
    "    bias,\n",
    "    variance,\n",
    "    training_time]]\n",
    "\n",
    "new_result_df = pd.DataFrame(new_result, columns=COLUMNS)\n",
    "concatenation = pd.concat([results_df, new_result_df], ignore_index=True)\n",
    "concatenation.to_csv(RESULTS_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_five = concatenation.tail(5)\n",
    "print(last_five)\n",
    "print(\"\\n---Metrics---\")\n",
    "print(last_five[[\"train_accuracy\", \"validation_accuracy\", \"bias\", \"variance\", \"training_time\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_FOLDER = \"../Models\"\n",
    "save_name = f\"{MODELS_FOLDER}/{model_name}-1.h5\"\n",
    "my_model.save(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeacd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a285d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
