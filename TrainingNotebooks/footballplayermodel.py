# -*- coding: utf-8 -*-
"""FootballPlayerModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hr09d81Xt3E63-g5y0SkswwNwZtgE47x

# Machine learning program "FootballPlayerModel"

*   Elemento de lista
*   Elemento de lista
"""

import tensorflow as tf
from tensorflow import keras
import pandas as pd
import matplotlib.pyplot as plt
import time
import numpy as np
from keras import regularizers
from keras.layers import BatchNormalization, Activation, Dropout, GaussianNoise
from keras.optimizers import Adam, SGD, RMSprop, Ftrl, Nadam
from sklearn.metrics import confusion_matrix
from keras.callbacks import LearningRateScheduler,TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from imblearn.over_sampling import ADASYN, RandomOverSampler, SMOTE

print("Tensorflow version: "+tf.__version__)


class main():
  def lrfn(epoch):
    #LEARNING RATE SCHEDULER:
    #Comienzo
    start_lr = 0.001  
    #Minimo a lo que puede llegar
    min_lr =  0.0000000000000000000001  
    #Maximo a lo que puede llegar
    max =  0.001
    max_lr = max * 1 
    #Numero de epocas a las que sube
    rampup_epochs = 300
    sustain_epochs = 0 
    #Numero de epocas en las que baja
    exp_decay = 0.9 

    """ 
    epoch: epoca de entrenamiento
    """
    if epoch < rampup_epochs: #Si la epoca no ha llegado al numero de epocas hasta las que se tiene que aumentar
      return (max_lr - start_lr)/rampup_epochs * epoch + start_lr #Aumentamos
    
    elif epoch < rampup_epochs + sustain_epochs: #Si la epoca ha llegado y se ha pasado, devolvemos el maximo
      return max_lr


    else: #Si la epoca ha llegado al numero de epocas hasta las que se tiene que aumentar
      return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr #Reducimos
  
  def train_evaluation(DA, n_epochs,batch_size, n_hidden):
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    tf.keras.backend.clear_session()

    """## Datasets"""

    INPUT_FILE = "FootballPlayer-Dataset/FootballPlayerRawDataset.csv"
    ATT_FILE = "FootballPlayer-Dataset/ImputedFootballPlayerPreparedCleanAttributes.csv"
    LABEL_FILE = "FootballPlayer-Dataset/ImputedFootballPlayerOneHotEncodedClasses.csv"

    attributes = pd.read_csv(ATT_FILE)
    label = pd.read_csv(LABEL_FILE)

    TRAIN_RATIO = 0.8

    n_instances = attributes.shape[0]
    n_train = int(n_instances*TRAIN_RATIO)
    n_dev = int((n_instances - n_train)/2)

    x_train = attributes.values[:n_train]
    t_train = label.values[:n_train]
    x_dev =   attributes.values[n_train:n_train + n_dev]
    t_dev =   label.values[n_train:n_train + n_dev]

    INPUTS =  x_train.shape[1]
    OUTPUTS = t_train.shape[1]

    NUM_TRAINING_EXAMPLES = int(round(x_train.shape[0]/1))
    NUM_DEV_EXAMPLES =      int(round(x_dev.shape[0]/1))

    print ("Number of training examples: ", NUM_TRAINING_EXAMPLES)
    print ("Number of examples for development test: ", NUM_DEV_EXAMPLES)
    # n_0 = 0
    # n_1 = 0
    # n_2 = 0
    # n_3 = 0
    # i = 0
    # while i < len(t_train):
    #   if t_train[i][0] == 1:
    #     n_0 += 1
    #   if t_train[i][1] == 1:
    #     n_1 += 1
    #   if t_train[i][2] == 1:
    #     n_2 += 1
    #   if t_train[i][3] == 1:
    #     n_3 += 1
    #   i += 1
    # print("Number of training examples of class 0: ",n_0)
    # print("Number of training examples of class 1: ",n_1)
    # print("Number of training examples of class 2: ",n_2)
    # print("Number of training examples of class 3: ",n_3)
    # print(t_train[0])

    #DATA AUGMENTATION - OVERSAMPLING
    #ada = ADASYN(sampling_strategy='not minority',random_state=42)
    if DA == True:
      ada = RandomOverSampler(sampling_strategy='all',random_state=42)
      x_train, t_train = ada.fit_resample(x_train, t_train)
      print('Resampled dataset shape', int(round(x_train.shape[0]/1)))

    #UNDERSAMPLING

    """## Model

    """

    n_neurons_per_hidden_layer = n_hidden #[256,128,64]
    learning_rate = 0.01

    model = keras.Sequential(name="my_model")
    #initializer = tf.keras.initializers.HeNormal()
    initializer = tf.keras.initializers.HeUniform()
    #initializer = tf.keras.initializers.RandomUniform(minval=-0.1,maxval=0.1)
    model.add(keras.layers.InputLayer(input_shape=(INPUTS,)))
    for neurons in n_neurons_per_hidden_layer:
      model.add(keras.layers.Dense(neurons,kernel_initializer=initializer,kernel_regularizer=regularizers.L1L2(l1=0.001,l2=0.1))) #kernel_initializer=initializer
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      #model.add(Dropout(0.3))
      #model.add(GaussianNoise(0.3))
    model.add(keras.layers.Dense(OUTPUTS, activation="softmax"))

    model.summary()

    model.compile(loss=tf.keras.losses.categorical_crossentropy,
                  #optimizer=SGD(learning_rate=learning_rate,decay=1e-6, momentum=0.90, nesterov=False),
                  #optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate,momentum=0.90,decay=1e-6),
                  #optimizer=RMSprop(learning_rate=learning_rate),
                  optimizer=Adam(learning_rate=learning_rate,decay=1e-6),
                  #optimizer=Nadam(learning_rate=learning_rate,decay=1e-6),
                  metrics=["categorical_accuracy"])

    """## Training"""

    n_epochs = n_epochs
    batch_size = batch_size
    start_time = time.perf_counter()

    #lr_scheduler = LearningRateScheduler(lambda epoch: main.lrfn(epoch), verbose=True)

    # rang = np.arange(n_epochs)
    # y = [lrfn(x) for x in rang]
    # plt.plot(rang, y)
    # plt.title('Learning Rate por epocas.')
    checkpoint_filepath = "C:/Users/Juanjo Flores/OneDrive/Desktop/MUIA/ARTIFICIAL NEURAL NETWORKS AND DEEP LEARNING"
    reduce_lr = ReduceLROnPlateau('val_categorical_accuracy', factor=0.1, patience=150, verbose=1)
    early_stop = EarlyStopping('val_categorical_accuracy', patience=500, verbose=1)
    checkpoint = ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,monitor='val_categorical_accuracy',mode='max',save_best_only=True)
    callbacks = [reduce_lr,checkpoint]#,reduce_lr]
    history = model.fit(x_train, t_train, 
                        batch_size = batch_size, 
                        epochs=n_epochs, 
                        validation_data = (x_dev, t_dev),
                        callbacks=callbacks,
                        verbose = 1)
    best_idx = int(np.argmax(history.history['val_categorical_accuracy']))
    best_value = np.max(history.history['val_categorical_accuracy'])
    print('Best validation model: epoch ' + str(best_idx+1), ' - val_categorical_accuracy ' + str(best_value))
    
    #cm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))
    
    return np.max(history.history['categorical_accuracy']), best_value
    """## Results"""

    # results = pd.DataFrame(history.history)
    # # results.plot(figsize = (8, 5))
    # # plt.grid(True)
    # # plt.xlabel ("Epochs")
    # # plt.ylabel ("Accuracy - Mean Log Loss")
    # # plt.gca().set_ylim(0, 1) # set the vertical range to [0,1]
    # # plt.show()

    #cm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))

    # print ("Error (training): ", 
    #       round((1 - results.categorical_accuracy.values[-1:][0])*100, 1), "%")
    # print ("Error (development test): ", 
    #       round((1 - results.val_categorical_accuracy.values[-1:][0])*100, 1), "%")
    # print ("Time: ", 
    #       round((time.perf_counter() - start_time)),"seconds")
